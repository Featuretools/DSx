{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New York City Taxi Ride Duration Prediction\n",
    "\n",
    "In this case study, we will build a predictive model to predict the duration of taxi ride. We will do the following steps:\n",
    "  * First install the dependencies\n",
    "  * Next load the data as pandas dataframe\n",
    "  * Define the outcome variable - the variable we are trying to predict.\n",
    "  * Build features using the [featuretools](featuretools.com) package that implements Deep Feature Synthesis. We will start with simple features and incrementally improve the feature definitions and examine the accuracy of the system.\n",
    "  \n",
    "\n",
    "\n",
    "Allocate at least 2-3 hours to go through this case study end-to-end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install Dependencies \n",
    "<p>If you have not done so already, download this repository <a href=\"https://github.com/Featuretools/DSx/archive/master.zip\">from git</a>. Once you have downloaded this archive, unzip it and cd into the directory from the command line. Next run the command ``./install_osx.sh`` if you are on a mac or ``./install_linux.sh`` if you are on linux. This should install all of the dependencies.</p>\n",
    "<p> If you are on a windows machine, open the requirements.txt folder and make sure to install each of the dependencies listed (featuretools, jupyter, pandas, sklearn, numpy) </p>\n",
    "<p> Once you have installed all of the dependencies, open this notebook. On Mac and Linux, navigate to the directory that you downloaded from git and run ``jupyter notebook`` to be taken to this notebook in your default web browser. When you open the NewYorkCity_taxi_case_study.ipynb file in the web browser, you can step through the code by clicking the ``Run`` button at the top of the page. If you have any questions for how to use <a href=\"http://jupyter.org/\">Jupyter</a>, refer to google or the discussion forum.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running the Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import featuretools as ft\n",
    "import utils\n",
    "from utils import load_nyc_taxi_data, compute_features, preview, feature_importances\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from featuretools.primitives import (Day, Hour, Minute, Month, Weekday, \n",
    "                                     Week, Weekend, Sum, Mean, Median, Std, Count)\n",
    "ft.__version__\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Download and load the raw data as pandas dataframes\n",
    "<p>If you have not yet downloaded the data it can be downloaded <a href=\"https://s3.amazonaws.com/mit-dsx-data/nyc-taxi-data.zip\">from S3</a>. Once you have downloaded the archive, unzip it and place the nyc-taxi-data folder in the same directory as this script. \n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialization complete\n",
      "Initialization complete\n",
      "Initialization complete\n",
      "Initialization complete\n",
      "start iteration\n",
      "done sorting\n",
      "start iteration\n",
      "done sorting\n",
      "start iteration\n",
      "done sorting\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "end inner loop\n",
      "end inner loop\n",
      "end inner loop\n",
      "Iteration 0, inertia 67.3424827888\n",
      "start iteration\n",
      "done sorting\n",
      "Iteration 0, inertia 64.0671364769\n",
      "start iteration\n",
      "Iteration 0, inertia 66.174031678\n",
      "done sorting\n",
      "start iteration\n",
      "done sorting\n",
      "Iteration 0, inertia 64.6004357218\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "end inner loop\n",
      "end inner loop\n",
      "end inner loop\n",
      "Iteration 1, inertia 63.1321597048\n",
      "start iteration\n",
      "done sorting\n",
      "Iteration 1, inertia 62.2435588287\n",
      "start iteration\n",
      "done sorting\n",
      "Iteration 1, inertia 60.3161685561\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 1, inertia 60.7431164512\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "end inner loop\n",
      "end inner loop\n",
      "Iteration 2, inertia 61.4483708957\n",
      "start iteration\n",
      "done sorting\n",
      "Iteration 2, inertia 58.9994359212\n",
      "start iteration\n",
      "done sorting\n",
      "Iteration 2, inertia 61.0068860701\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "end inner loop\n",
      "end inner loop\n",
      "Iteration 2, inertia 59.1850212449\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 3, inertia 58.4742926219\n",
      "start iteration\n",
      "done sorting\n",
      "Iteration 3, inertia 60.4013034028\n",
      "start iteration\n",
      "done sorting\n",
      "Iteration 3, inertia 58.19435464\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 3, inertia 60.1353429468\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "end inner loop\n",
      "end inner loop\n",
      "Iteration 4, inertia 58.1540571198\n",
      "start iteration\n",
      "done sorting\n",
      "Iteration 4, inertia 59.6123976784\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 4, inertia 57.7003730501\n",
      "start iteration\n",
      "done sorting\n",
      "Iteration 4, inertia 59.2967172298\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "end inner loop\n",
      "end inner loop\n",
      "Iteration 5, inertia 57.9741108137\n",
      "start iteration\n",
      "done sorting\n",
      "Iteration 5, inertia 58.9516355716\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "end inner loop\n",
      "Iteration 5, inertia 57.3585242185\n",
      "start iteration\n",
      "done sorting\n",
      "Iteration 6, inertia 57.8513935408\n",
      "start iteration\n",
      "done sorting\n",
      "Iteration 5, inertia 58.9018766568\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "end inner loop\n",
      "end inner loop\n",
      "Iteration 6, inertia 58.5140524698\n",
      "done sorting\n",
      "start iteration\n",
      "Iteration 6, inertia 57.1090884933\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 7, inertia 57.7642326221\n",
      "start iteration\n",
      "done sorting\n",
      "Iteration 6, inertia 58.7078043174\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "end inner loop\n",
      "end inner loop\n",
      "Iteration 7, inertia 58.2032544591\n",
      "start iteration\n",
      "done sorting\n",
      "Iteration 7, inertia 56.8971919552\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 8, inertia 57.6773704195\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 7, inertia 58.5530554764\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n"
     ]
    }
   ],
   "source": [
    "trips, passenger_cnt, vendors = load_nyc_taxi_data()\n",
    "preview(trips,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ``trips`` table has the following fields\n",
    "* ``id`` which uniquely identifies the trip\n",
    "* ``vendor_id`` is the taxi cab company - in our case study we have data from three different cab companies\n",
    "* ``pickup_datetime`` the time stamp for pickup\n",
    "* ``dropoff_datetime`` the time stamp for drop-off\n",
    "* ``passenger_count`` the number of passengers for the trip\n",
    "* ``trip_distance`` total distance of the trip in miles \n",
    "* ``pickup_longitude`` the longitude for pickup\n",
    "* ``pickup_latitude`` the latitude for pickup\n",
    "* ``dropoff_longitude``the longitude of dropoff \n",
    "* ``dropoff_latitude`` the latitude of dropoff\n",
    "* ``payment_type`` A numeric code signifying how the passenger paid for the trip. 1= Credit card 2= Cash 3= No charge 4= Dispute 5= Unknown 6= Voided\n",
    "* ``trip_duration`` this is the duration we would like to predict using other fields "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Prepare the Data\n",
    "Lets create entities and relationships. The three entities in this data are \n",
    "* trips \n",
    "* vendors (these are the cab companies)\n",
    "* passenger_cnt (a simple entity that has the unique number of passenger counts 1-8)\n",
    "\n",
    "This data has the following relationships\n",
    "* Vendors --> trips (the same vendor can have multiple trips - vendors is the ``parent_entity`` and trips it the child entity\n",
    "* passenger_cnt --> trips (the same passenger_cnt can appear in multiple trips. passenger_cnt is the ``parent_entity`` and trips is the child entity. \n",
    "\n",
    "In <a <href=\"https://www.featuretools.com/\"><featuretools (automated feature engineering software package)/></a>, we specify the list of entities and relationships as follows: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "entities = {\n",
    "        \"trips\": (trips, \"id\", 'pickup_datetime' ),\n",
    "        \"vendors\": (vendors, \"vendor_id\"),\n",
    "        \"passenger_cnt\": (passenger_cnt,\"passenger_count\")\n",
    "        }\n",
    "\n",
    "relationships = [(\"vendors\", \"vendor_id\",\"trips\", \"vendor_id\"), \n",
    "                (\"passenger_cnt\", \"passenger_count\",\"trips\", \"passenger_count\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>We specify the time for each instance of the target_entity, in this case ``trips`` to calculate features. The timestamp represents the last time data can be used for calculating features by DFS. This is specified using a dataframe of cutoff time. This cutoff time for each trip is the pickup time. We want to have a minimum amount of data, so we only use trips after January 12th, 2016</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cutoff_time = trips[['id', 'pickup_datetime']]\n",
    "cutoff_time = cutoff_time[cutoff_time['pickup_datetime'] > \"2016-01-12\"]\n",
    "preview(cutoff_time, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Create baseline features using DFS \n",
    "<p>Instead of manually creating features, such as \"<b>month of pickup_datetime</b>\", we can let featuretools come up with them. </p> \n",
    "\n",
    "Featuretools does this by \n",
    "* interpret the types of variables - categorical, numeric and others. We can override this interpretation by specifying the types. In this case study, we wanted <b>passenger_count</b> to be a type of Ordinal, and <b>vendor_id</b> to be of type Categorical. This override occured while loading in the csv files.</p>\n",
    "* then based on the primitives we specify, it matches up the columns to which those primitives can be applied. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create transform features using transform primitives\n",
    "\n",
    "As we described in the video, features fall into two major categories, ``transform`` and ``aggregate``. In featureools, we can create transform features by specifying ``transform`` primitives. Below we specify a ``transform`` primitive called ``weekend`` and here is what it does:\n",
    "\n",
    "* It can be applied to any ``datetime`` column in the data. \n",
    "* For each entry in the column, it assess if it is a ``weekend`` and returns a boolean. \n",
    "\n",
    "In this specific data, there are two ``datetime`` columns ``pickup_datetime`` and ``dropoff_datetime``. The tool automatically creates features using the primitive and these two columns as shown below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trans_primitives = [Weekend]\n",
    "\n",
    "features = ft.dfs(entities=entities,\n",
    "                  relationships=relationships,\n",
    "                  target_entity=\"trips\",\n",
    "                  trans_primitives=trans_primitives,\n",
    "                  agg_primitives=[],\n",
    "                  features_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Here are the features created.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print len(features)\n",
    "features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's compute the features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_matrix = compute_features(features, cutoff_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Build the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To build a model,\n",
    "* we first seperate the data into a porition for ``training`` (75% in this case) and a portion for ``testing`` \n",
    "* We also get the log of the trip duration so that a more linear relationship can be found.\n",
    "* We use ``GradientBoostingRegressor`` to train a model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# separates the whole feature matrix into train data feature matrix, \n",
    "# train data labels, and test data feature matrix \n",
    "X_train, y_train, X_test, y_test = utils.get_train_test_fm(feature_matrix,.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = GradientBoostingRegressor()\n",
    "model.fit(X_train,y_train)\n",
    "model.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: Adding more Transform Primitives\n",
    "\n",
    "* Add ``Minute``, ``Hour``, ``Week``, ``Month``, ``Weekday`` primitives\n",
    "* All these transform primitives apply to ``datetime`` column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trans_primitives = [Minute, Hour, Day, Week, Month, Weekday, Weekend]\n",
    "\n",
    "features = ft.dfs(entities=entities,\n",
    "                   relationships=relationships,\n",
    "                   target_entity=\"trips\",\n",
    "                   trans_primitives=trans_primitives,\n",
    "                   agg_primitives=[],\n",
    "                   features_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print len(features)\n",
    "features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's compute the features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_matrix = compute_features(features, cutoff_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preview(feature_matrix, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6: Build the new model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# separates the whole feature matrix into train data feature matrix,\n",
    "# train data labels, and test data feature matrix \n",
    "X_train, y_train, X_test, y_test = utils.get_train_test_fm(feature_matrix,.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GradientBoostingRegressor()\n",
    "model.fit(X_train,y_train)\n",
    "model.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 7: Add Aggregation Primitives\n",
    "\n",
    "Now let's add aggregation primitives. These primitives will generate features for the parent entities in this case both ``vendors`` and ``passenger_cnt`` and then add them to the trips entity (which is the entity for which we are trying to make prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trans_primitives = [Minute, Hour, Day, Week, Month, Weekday, Weekend]\n",
    "aggregation_primitives = [Count, Sum, Mean, Median, Std]\n",
    "\n",
    "features = ft.dfs(entities=entities,\n",
    "                   relationships=relationships,\n",
    "                   target_entity=\"trips\",\n",
    "                   trans_primitives=trans_primitives,\n",
    "                   agg_primitives=aggregation_primitives,\n",
    "                   features_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print len(features)\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_matrix = compute_features(features, cutoff_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preview(feature_matrix, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 8: Build the new model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# separates the whole feature matrix into train data feature matrix,\n",
    "# train data labels, and test data feature matrix \n",
    "X_train, y_train, X_test, y_test = utils.get_train_test_fm(feature_matrix,.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = GradientBoostingRegressor()\n",
    "model.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 9: Evalute on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "y_pred[5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Additional Analysis\n",
    "<p>Let's look at how important each feature was for the model.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importances(model, feature_matrix.columns.tolist(), n=15)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
